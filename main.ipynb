{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb519ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (0.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.24.1)\n",
      "Requirement already satisfied: transformer_lens==2.11.0 in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
      "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (1.7.0)\n",
      "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (0.14.1)\n",
      "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (0.0.3)\n",
      "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (3.6.0)\n",
      "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (0.0.3)\n",
      "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (0.3.2)\n",
      "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (2.3.0)\n",
      "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (14.0.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (0.2.0)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (4.67.1)\n",
      "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (4.52.4)\n",
      "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (4.4.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (4.14.0)\n",
      "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.10/dist-packages (from transformer_lens==2.11.0) (0.20.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens==2.11.0) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens==2.11.0) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens==2.11.0) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens==2.11.0) (0.33.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer_lens==2.11.0) (0.5.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens==2.11.0) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens==2.11.0) (0.3.8)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens==2.11.0) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens==2.11.0) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer_lens==2.11.0) (0.70.16)\n",
      "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from jaxtyping>=0.2.11->transformer_lens==2.11.0) (0.1.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens==2.11.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens==2.11.0) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer_lens==2.11.0) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens==2.11.0) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer_lens==2.11.0) (2.16.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens==2.11.0) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer_lens==2.11.0) (0.21.1)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens==2.11.0) (8.2.1)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens==2.11.0) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens==2.11.0) (3.11.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens==2.11.0) (6.31.1)\n",
      "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens==2.11.0) (2.11.7)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens==2.11.0) (2.30.0)\n",
      "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer_lens==2.11.0) (1.3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0) (3.12.13)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens==2.11.0) (4.0.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.23.0->transformer_lens==2.11.0) (1.1.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens==2.11.0) (0.1.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens==2.11.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens==2.11.0) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens==2.11.0) (0.4.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer_lens==2.11.0) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens==2.11.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens==2.11.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens==2.11.0) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens==2.11.0) (2022.12.7)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens==2.11.0) (1.20.1)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens==2.11.0) (5.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch einops numpy transformer_lens==2.11.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56bcd892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import einops\n",
    "import numpy as np\n",
    "from transformer_lens import (\n",
    "    ActivationCache,\n",
    "    FactoredMatrix,\n",
    "    HookedTransformer,\n",
    "    HookedTransformerConfig,\n",
    "    utils,\n",
    ")\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "import tqdm\n",
    "import csv\n",
    "import functools\n",
    "from typing import DefaultDict\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0f5606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a9c26d3f8241f7b5d4e83740f5149c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed8bdabe4bc4914a19a7408e81ed41c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3155c1c4cbd9446d8f34a8209ca31b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6da10a985d6f48cd9b7c7613785ef5fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f65dfb511d6440d6b4cc2cfeee6ef246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "125423b569f14f05ad26539c13fb130a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e98bfb5f254a91b20cb3184d957bf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-XL into HookedTransformer\n",
      "Moving model to device:  cuda\n"
     ]
    }
   ],
   "source": [
    "gpt2_small = HookedTransformer.from_pretrained('gpt2').to(device)\n",
    "gpt2_xl = HookedTransformer.from_pretrained('gpt2-XL').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "41b9a937",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gpt2_xl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a69f97f",
   "metadata": {},
   "source": [
    "### CAA [Sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5d769b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "with open('sentences.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        sentences.append(tuple(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c226ede2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['blocks.0.hook_resid_pre', 'blocks.0.hook_resid_mid', 'blocks.1.hook_resid_pre', 'blocks.1.hook_resid_mid', 'blocks.2.hook_resid_pre', 'blocks.2.hook_resid_mid', 'blocks.3.hook_resid_pre', 'blocks.3.hook_resid_mid', 'blocks.4.hook_resid_pre', 'blocks.4.hook_resid_mid', 'blocks.5.hook_resid_pre', 'blocks.5.hook_resid_mid', 'blocks.6.hook_resid_pre', 'blocks.6.hook_resid_mid', 'blocks.7.hook_resid_pre', 'blocks.7.hook_resid_mid', 'blocks.8.hook_resid_pre', 'blocks.8.hook_resid_mid', 'blocks.9.hook_resid_pre', 'blocks.9.hook_resid_mid', 'blocks.10.hook_resid_pre', 'blocks.10.hook_resid_mid', 'blocks.11.hook_resid_pre', 'blocks.11.hook_resid_mid', 'blocks.12.hook_resid_pre', 'blocks.12.hook_resid_mid', 'blocks.13.hook_resid_pre', 'blocks.13.hook_resid_mid', 'blocks.14.hook_resid_pre', 'blocks.14.hook_resid_mid', 'blocks.15.hook_resid_pre', 'blocks.15.hook_resid_mid', 'blocks.16.hook_resid_pre', 'blocks.16.hook_resid_mid', 'blocks.17.hook_resid_pre', 'blocks.17.hook_resid_mid', 'blocks.18.hook_resid_pre', 'blocks.18.hook_resid_mid', 'blocks.19.hook_resid_pre', 'blocks.19.hook_resid_mid', 'blocks.20.hook_resid_pre', 'blocks.20.hook_resid_mid', 'blocks.21.hook_resid_pre', 'blocks.21.hook_resid_mid', 'blocks.22.hook_resid_pre', 'blocks.22.hook_resid_mid', 'blocks.23.hook_resid_pre', 'blocks.23.hook_resid_mid', 'blocks.24.hook_resid_pre', 'blocks.24.hook_resid_mid', 'blocks.25.hook_resid_pre', 'blocks.25.hook_resid_mid', 'blocks.26.hook_resid_pre', 'blocks.26.hook_resid_mid', 'blocks.27.hook_resid_pre', 'blocks.27.hook_resid_mid', 'blocks.28.hook_resid_pre', 'blocks.28.hook_resid_mid', 'blocks.29.hook_resid_pre', 'blocks.29.hook_resid_mid', 'blocks.30.hook_resid_pre', 'blocks.30.hook_resid_mid', 'blocks.31.hook_resid_pre', 'blocks.31.hook_resid_mid', 'blocks.32.hook_resid_pre', 'blocks.32.hook_resid_mid', 'blocks.33.hook_resid_pre', 'blocks.33.hook_resid_mid', 'blocks.34.hook_resid_pre', 'blocks.34.hook_resid_mid', 'blocks.35.hook_resid_pre', 'blocks.35.hook_resid_mid', 'blocks.36.hook_resid_pre', 'blocks.36.hook_resid_mid', 'blocks.37.hook_resid_pre', 'blocks.37.hook_resid_mid', 'blocks.38.hook_resid_pre', 'blocks.38.hook_resid_mid', 'blocks.39.hook_resid_pre', 'blocks.39.hook_resid_mid', 'blocks.40.hook_resid_pre', 'blocks.40.hook_resid_mid', 'blocks.41.hook_resid_pre', 'blocks.41.hook_resid_mid', 'blocks.42.hook_resid_pre', 'blocks.42.hook_resid_mid', 'blocks.43.hook_resid_pre', 'blocks.43.hook_resid_mid', 'blocks.44.hook_resid_pre', 'blocks.44.hook_resid_mid', 'blocks.45.hook_resid_pre', 'blocks.45.hook_resid_mid', 'blocks.46.hook_resid_pre', 'blocks.46.hook_resid_mid', 'blocks.47.hook_resid_pre', 'blocks.47.hook_resid_mid']\n"
     ]
    }
   ],
   "source": [
    "test_cache = model.run_with_cache('')[1]\n",
    "keys = []\n",
    "for key in test_cache:\n",
    "    if ('resid_pre' in key) or ('resid_mid' in key):\n",
    "        keys.append(key)\n",
    "print(keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a9e0f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_steering_sentences = t.zeros(len(keys), model.cfg.d_model).to(device)\n",
    "\n",
    "for (english, spanish) in sentences:\n",
    "    cache_eng = model.run_with_cache(english)[1]\n",
    "    cache_spa = model.run_with_cache(spanish)[1]\n",
    "\n",
    "    for idx, key in enumerate(keys):\n",
    "        vec_eng = cache_eng[key].squeeze()[-1] #ie. vector at residual-stream-of-last-token\n",
    "        vec_spa = cache_spa[key].squeeze()[-1]\n",
    "        steering = vec_spa - vec_eng\n",
    "        accum_steering_sentences[idx] += steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "426a310e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " of\n",
      " of\n",
      " of\n",
      " of\n",
      " of\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " water\n",
      " of\n",
      " of\n",
      " of\n",
      " of\n",
      " of\n",
      " of\n",
      " of\n",
      " of\n",
      " of\n",
      " of\n",
      " of\n",
      " of\n",
      " por\n",
      " of\n",
      " un\n",
      " un\n",
      " of\n",
      " of\n",
      " un\n",
      " un\n",
      " un\n",
      " un\n",
      " por\n",
      " por\n",
      " un\n",
      " un\n",
      " un\n",
      " un\n",
      " por\n",
      " por\n",
      " un\n",
      " un\n",
      " por\n",
      " por\n",
      " por\n",
      " por\n"
     ]
    }
   ],
   "source": [
    "steering_vecs_sentences = accum_steering_sentences / 50\n",
    "\n",
    "def sentences_hook(res_vector, hook, steering):\n",
    "    return res_vector + steering\n",
    "\n",
    "for idx, key in enumerate(keys):\n",
    "    partial_hook = functools.partial(sentences_hook, steering=steering_vecs_sentences[idx])\n",
    "    logits = model.run_with_hooks(\n",
    "        'The horse drank some',\n",
    "        return_type = 'logits',\n",
    "        fwd_hooks = [(key, partial_hook)],\n",
    "    )\n",
    "    out = logits.squeeze().argmax(-1)[-1]\n",
    "    print(model.to_string(out))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1158938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {' water': 56,\n",
       "             'ic': 1,\n",
       "             ' of': 95,\n",
       "             'ones': 1,\n",
       "             ' words': 1,\n",
       "             '.': 67,\n",
       "             ',': 79,\n",
       "             ' SE': 5,\n",
       "             ' BE': 4,\n",
       "             ' =': 5,\n",
       "             ' /': 2,\n",
       "             ' w': 4,\n",
       "             ' Sur': 2,\n",
       "             ' en': 69,\n",
       "             ' su': 5,\n",
       "             ' un': 8,\n",
       "             ' se': 5,\n",
       "             ' es': 11,\n",
       "             ' non': 1,\n",
       "             ' el': 34,\n",
       "             ' de': 18,\n",
       "             ' este': 13,\n",
       "             ' the': 5,\n",
       "             ' some': 13,\n",
       "             '\\n': 47,\n",
       "             ' (': 43,\n",
       "             ' est': 34,\n",
       "             ' ur': 4,\n",
       "             ' \"': 4,\n",
       "             'Mi': 3,\n",
       "             ' entr': 6,\n",
       "             'senal': 4,\n",
       "             ' o': 30,\n",
       "             ' u': 3,\n",
       "             ' cop': 2,\n",
       "             ' or': 3,\n",
       "             ' te': 1,\n",
       "             ' com': 8,\n",
       "             ' por': 134,\n",
       "             ' l': 2,\n",
       "             ' p': 1,\n",
       "             ' L': 3,\n",
       "             ' #': 2,\n",
       "             ' people': 1,\n",
       "             ' que': 25,\n",
       "             'n': 3,\n",
       "             ' a': 1,\n",
       "             ':': 3,\n",
       "             ' e': 1,\n",
       "             ' au': 4,\n",
       "             ' n': 1,\n",
       "             ' English': 1,\n",
       "             ' Spanish': 4,\n",
       "             ' mi': 17,\n",
       "             ' to': 2,\n",
       "             ' na': 1,\n",
       "             ' pr': 1,\n",
       "             ' h': 1,\n",
       "             '--': 9,\n",
       "             '...': 1,\n",
       "             ' y': 13,\n",
       "             '—': 4,\n",
       "             ' �': 6,\n",
       "             ' Por': 5,\n",
       "             ' s': 1,\n",
       "             ' no': 3,\n",
       "             ' Mexican': 1,\n",
       "             ' aqu': 2,\n",
       "             ' compr': 3,\n",
       "             ' et': 1,\n",
       "             ' unt': 1,\n",
       "             ' N': 6})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_hook(res_vector, hook, steering):\n",
    "    return res_vector + 5 * steering\n",
    "\n",
    "preds = DefaultDict(int)\n",
    "\n",
    "for (english, spanish) in sentences[:10]:\n",
    "    cache_eng = gpt2_xl.run_with_cache(english)[1]\n",
    "    cache_spa = gpt2_xl.run_with_cache(spanish)[1]\n",
    "\n",
    "    for idx, key in enumerate(keys):\n",
    "        vec_eng = cache_eng[key].squeeze()[-1]\n",
    "        vec_spa = cache_spa[key].squeeze()[-1]\n",
    "        steering = vec_spa - vec_eng\n",
    "\n",
    "        my_hook = functools.partial(test_hook, steering = steering)\n",
    "        logits = gpt2_xl.run_with_hooks(\n",
    "            'The horse drank some',\n",
    "            return_type = 'logits',\n",
    "            fwd_hooks = [(key, my_hook)]\n",
    "        )\n",
    "\n",
    "        pred = logits.squeeze().argmax(dim=-1)[-1]\n",
    "        preds[gpt2_xl.to_string(pred)] += 1\n",
    "\n",
    "# preds should have total of 24 * 50\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d010cca",
   "metadata": {},
   "source": [
    "### CAA [Words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "32bb79c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "with open('words.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        words.append(tuple(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a0c1192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'.': 20,\n",
       "             'asa': 6,\n",
       "             ',': 187,\n",
       "             \"'s\": 51,\n",
       "             ' de': 192,\n",
       "             ' Grande': 18,\n",
       "             ' Blanc': 2,\n",
       "             'ro': 5,\n",
       "             'ze': 4,\n",
       "             '-': 95,\n",
       "             '/': 3,\n",
       "             'pe': 2,\n",
       "             'ff': 17,\n",
       "             'ppe': 4,\n",
       "             '\\n': 8,\n",
       "             'oso': 11,\n",
       "             '\\xad': 6,\n",
       "             'iac': 6,\n",
       "             'onga': 6,\n",
       "             'pload': 2,\n",
       "             'e': 2,\n",
       "             ' e': 9,\n",
       "             ' o': 4,\n",
       "             ' d': 2,\n",
       "             ' dé': 2,\n",
       "             'ux': 2,\n",
       "             'ú': 1,\n",
       "             'ister': 1,\n",
       "             'í': 2,\n",
       "             ' (': 12,\n",
       "             ' certainty': 1,\n",
       "             '�': 27,\n",
       "             ' con': 3,\n",
       "             ' del': 39,\n",
       "             ' por': 2,\n",
       "             'ña': 12,\n",
       "             'igo': 1,\n",
       "             ' U': 5,\n",
       "             'rer': 23,\n",
       "             ' to': 27,\n",
       "             'ment': 10,\n",
       "             'ly': 7,\n",
       "             ' apologized': 4,\n",
       "             'idad': 23,\n",
       "             ' para': 1,\n",
       "             ' la': 2,\n",
       "             'o': 4,\n",
       "             ' a': 4,\n",
       "             ' ent': 1,\n",
       "             ' the': 1,\n",
       "             ' Garc': 20,\n",
       "             ' n': 2,\n",
       "             ' and': 4,\n",
       "             ' is': 1,\n",
       "             'emon': 2,\n",
       "             'ol': 13,\n",
       "             \"'\": 6,\n",
       "             'ue': 1,\n",
       "             'ors': 12,\n",
       "             'iz': 6,\n",
       "             ' [': 3,\n",
       "             'ised': 11})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def test_hook(res_vector, hook, steering):\n",
    "    return res_vector + (5 * steering)\n",
    "\n",
    "preds = DefaultDict(int)\n",
    "\n",
    "for (english, spanish) in words[:10]:\n",
    "    cache_eng = model.run_with_cache(english)[1]\n",
    "    cache_spa = model.run_with_cache(spanish)[1]\n",
    "\n",
    "    for idx, key in enumerate(keys):\n",
    "        vec_eng = cache_eng[key].squeeze()[-1]\n",
    "        vec_spa = cache_spa[key].squeeze()[-1]\n",
    "        steering = vec_spa - vec_eng\n",
    "\n",
    "        my_hook = functools.partial(test_hook, steering = steering)\n",
    "        logits = model.run_with_hooks(\n",
    "            'The horse drank some',\n",
    "            return_type = 'logits',\n",
    "            fwd_hooks = [(key, my_hook)]\n",
    "        )\n",
    "\n",
    "        pred = logits.squeeze().argmax(dim=-1)[-1]\n",
    "        preds[model.to_string(pred)] += 1\n",
    "\n",
    "# preds should have total of 24 * 50\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671dd100",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum_steering_words = t.zeros_like(accum_steering_sentences)\n",
    "\n",
    "for (english, spanish) in words:\n",
    "    cache_eng = gpt2_small.run_with_cache(english)[1]\n",
    "    cache_spa = gpt2_small.run_with_cache(spanish)[1]\n",
    "\n",
    "    for idx, key in enumerate(keys):\n",
    "        vec_eng = cache_eng[key].squeeze()[-1]\n",
    "        vec_spa = cache_spa[key].squeeze()[-1]\n",
    "        steering = vec_spa - vec_eng\n",
    "        accum_steering_words[idx] += steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb34f72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vecs_words = accum_steering_words / 50\n",
    "\n",
    "def words_hook(res_vector, hook, steering):\n",
    "    return res_vector + steering\n",
    "\n",
    "for idx, key in enumerate(keys):\n",
    "    partial_hook = functools.partial(words_hook, steering=steering_vecs_words[idx])\n",
    "    logits = gpt2_small.run_with_hooks(\n",
    "        'The horse drank some',\n",
    "        return_type = 'logits',\n",
    "        fwd_hooks = [(key, partial_hook)],\n",
    "    )\n",
    "    out = logits.squeeze().argmax(-1)[-1]\n",
    "    print(gpt2_small.to_string(out))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e79b38",
   "metadata": {},
   "source": [
    "### CAA [Paragraph]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b93e49",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
